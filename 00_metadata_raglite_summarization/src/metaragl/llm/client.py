from typing import Optional
import json
import urllib.request
import urllib.error
from ..config import settings

class BaseLLM:
    def generate(self, prompt: str) -> str:
        raise NotImplementedError

class MockLLM(BaseLLM):
    """
    Offline predictable fallback: uses simplistic heuristics.
    Great for testing the pipeline plumbing without a real model.
    """
    def generate(self, prompt: str) -> str:
        # Extremely naive extraction demo
        lower = prompt.lower()
        sensitivity = "sensitivity:internal" if "internal" in lower else "sensitivity:public"
        topic = "topic:food-safety" if "inspection" in lower else "topic:ag-economics"

        out = {
            "title": "Auto-generated metadata record",
            "doc_type": "memo" if "document type hint: memo" in lower else "unknown",
            "abstract": "Short summary generated by mock backend for local testing.",
            "topics": [topic],
            "programs": [],
            "geographies": ["geo:midwest"] if "midwest" in lower else [],
            "sensitivity": sensitivity,
            "keywords": ["inspection", "corrective action"] if "inspection" in lower else ["markets"],
            "evidence_snippets": ["intended for internal operational use" if sensitivity.endswith("internal") else "publicly releasable"]
        }
        return json.dumps(out)

class OllamaLLM(BaseLLM):
    def __init__(self, base_url: str, model: str):
        self.base_url = base_url.rstrip("/")
        self.model = model

    def generate(self, prompt: str) -> str:
        # Uses Ollama generate endpoint
        url = f"{self.base_url}/api/generate"
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "format": "json",
            "options": {"temperature": 0.2}
        }
        data = json.dumps(payload).encode("utf-8")
        req = urllib.request.Request(url, data=data, headers={"Content-Type": "application/json"})
        try:
            with urllib.request.urlopen(req, timeout=120) as resp:
                body = resp.read().decode("utf-8")
                obj = json.loads(body)
                resp = (obj.get("response") or "").strip()
                if not resp:
                    raise RuntimeError(f"Ollama returned an empty response. Full body: {body[:5000]}")
                return resp
                # return obj.get("response", "").strip()
        except urllib.error.URLError as e:
            raise RuntimeError(f"Ollama call failed: {e}")

def get_llm() -> BaseLLM:
    if settings.llm_backend == "ollama":
        return OllamaLLM(settings.ollama_base_url, settings.ollama_model)
    return MockLLM()