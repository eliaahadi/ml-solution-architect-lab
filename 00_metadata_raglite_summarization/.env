# LLM backend
# Use `mock` for a fully offline, predictable demo.
# METARAGL_LLM_BACKEND=mock

# If/when you install Ollama, switch the backend to `ollama` and uncomment these:
# METARAGL_LLM_BACKEND=ollama
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.1


METARAGL_LLM_BACKEND=ollama
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=llama3.1:8b